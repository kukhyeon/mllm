<h1 align="center">
  <em>IGNITE</em>
</h1>

<h3 align="center">
Inference Governed by Nested lazy Ignition for Thermal Efficiency
</h3>

<h4 align="center">
| Arm CPU | X86 CPU |
</h4>

<h4 align="center">

[![Paper](https://img.shields.io/badge/view-paper-blue)](#)
[![Android App](https://img.shields.io/badge/android-app-pink)](https://github.com/kjh2159/ChatBotApp/)
[![Actions Status](https://github.com/UbiquitousLearning/mllm/workflows/Tests/badge.svg)](https://github.com/kjh2159/mllm/actions)
</h4>

- Plain C/C++ implementation without dependencies
- Optimized for multimodal LLMs like Qwen2-VL and LLaVA
- Supported: ARM NEON, x86 AVX2, etc
- Various quantization schemes
- End-to-end Android app demo
- Advanced support for *IGNITE*
  - Phase-level DVFS control (CPU/RAM)
  - Phase-level pause injection
  - Layer-level pause injection


<!-- ### Contents
- [Android Demo](#android-demo)
- [Support models](#support-models)
- [Quick Start](#quick-start)
    - [Get the Code](#get-the-code)
    - [Check prerequisites](#check-prerequisites)
    - [Run with the CPU of Android](#run-with-the-cpu-of-android)
    - [Run with the CPU on Termux](#run-with-the-cpu-on-termux)
    - [Run for Linux](#run-for-linux)
- [Customization](#customization)
    - [Convert models](#convert-models)
    - [Convert vocabulary](#convert-vocabulary)
    - [Quantize models](#quantize-models)
- [Acknowledgments](#acknowledgments)
- [License](#license) -->

## üöÇ Support engines

### Announcement

**Now, mllm engine is only supported. We are preparing llama.cpp support for public version.**

## üéüÔ∏è Support models

### Language models

| Model                                                                       | CPU <br> FP32 | CPU <br> INT4  | *IGNITE* |
|-----------------------------------------------------------------------------|---------------|----------------|----------|
| [LLaMA3.2 1B](https://github.com/meta-llama/llama3) | [‚úîÔ∏è](https://huggingface.co/mllmTeam/llama-3.2-1b-mllm/tree/main)  | [‚úîÔ∏è](https://huggingface.co/mllmTeam/llama-3.2-1b-mllm/tree/main)   | ‚≠ï |
| [LLaMA3.2 3B](https://github.com/meta-llama/llama3) | [‚úîÔ∏è](https://huggingface.co/mllmTeam/llama-3.2-3b-mllm/tree/main)  | [‚úîÔ∏è](https://huggingface.co/mllmTeam/llama-3.2-3b-mllm/tree/main) | ‚≠ï |
| [Qwen1.5 0.5B](https://github.com/QwenLM/Qwen) | [‚úîÔ∏è](https://huggingface.co/mllmTeam/qwen-1.5-0.5b-mllm/tree/main)  | [‚úîÔ∏è](https://huggingface.co/mllmTeam/qwen-1.5-0.5b-mllm/tree/main) | ‚≠ï |
| [Qwen1.5 1.8B](https://github.com/QwenLM/Qwen) | [‚úîÔ∏è](https://huggingface.co/mllmTeam/qwen-1.5-1.8b-mllm/tree/main)  | [‚úîÔ∏è](https://huggingface.co/mllmTeam/qwen-1.5-1.8b-mllm/tree/main) | ‚≠ï |
| [Qwen2.5 0.5B](https://github.com/QwenLM/Qwen3) | [‚úîÔ∏è](https://huggingface.co/kjh2159/Qwen2.5-0.5B-MLLM/tree/main)  | [‚úîÔ∏è](https://huggingface.co/kjh2159/Qwen2.5-0.5B-MLLM/tree/main) | ‚≠ï |
| [Qwen2.5 1.5B](https://github.com/QwenLM/Qwen3) | [‚úîÔ∏è](https://huggingface.co/mllmTeam/qwen-2.5-1.5b-mllm/tree/main)  | [‚úîÔ∏è](https://huggingface.co/mllmTeam/qwen-2.5-1.5b-mllm/tree/main) | ‚≠ï |
| [Qwen3 0.6B](https://github.com/QwenLM/Qwen3) | [‚úîÔ∏è](https://huggingface.co/mllmTeam/qwen-3-0.6b-mllm/tree/main)  | [‚úîÔ∏è](https://huggingface.co/mllmTeam/qwen-3-0.6b-mllm/tree/main) | ‚≠ï |
| [Qwen3 1.7B](https://github.com/QwenLM/Qwen3) | [‚úîÔ∏è](https://huggingface.co/kjh2159/Qwen3-1.7B-MLLM/tree/main)  | [‚úîÔ∏è](https://huggingface.co/kjh2159/Qwen3-1.7B-MLLM/tree/main) | ‚≠ï |

> For other models, please refer to the following two hugging face repositories (extension type is .mllm): [mllm](https://huggingface.co/mllmTeam/models) and [*IGNITE*](https://huggingface.co/kjh2159/models/).

## üöÄ Quick Start

### 1. Get the code

```bash
git clone https://github.com/kjh2159/mllm
cd mllm
```

### 2. Check prerequisites

Building mllm requires following tools:

- gcc(11.4+) / clang (11.0+)
- CMake >= 3.18
- Android NDK Toolchains >= 26


> Note that building OpenMP libs on macOS may fail due to Apple LLVM compiler, so we disable OpenMP on macOS by default, you may experience slower performance on macOS. Build mllm is more recommended on Linux.

### 3-1. Run with CPU on Android shell

*`NOTE:` This project requires to root an android phone. Also, depending on your phones, RAM DVFS may not be supported. Please first check if your phone is available for RAM DVFS*

#### a. *Build*

  ```bash
  export ANDROID_NDK=/path/to/your/ndk
  cd scripts
  ./build_android.sh
  ```

#### b. *Download model: Qwen1.5 0.5B*

Download the model from [here](https://huggingface.co/mllmTeam/qwen-1.5-0.5b-mllm/tree/main) and place the model file in the directory of `models`, or using the following instructions

```bash
mkdir ../models && cd ../models
# Download qwen-1.5-0.5b-q4_k.mllm
wget https://huggingface.co/mllmTeam/qwen-1.5-0.5b-mllm/resolve/main/qwen-1.5-0.5b-q4_k.mllm?download=true  -O qwen-1.5-0.5b-q4_k.mllm
```

#### c. *Run remote on Android Phone*

```bash
sh scripts-arm/run-setup.sh
sh scripts-arm/run-remote.sh
```

Result are as followed:

```
[Q] Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?
[A] K Knox County Regional Airport is located in Knox County, Maine

===========================================
            Inference
-------------------------------------------
  Load time: 0.540094 s
  Prefilling speed: 75.0717 tokens/s
  Decoding speed: 26.0652 tokens/s
===========================================
```


### 3-2. Run with CPU on Termux

*`NOTE:` This project requires to root an android phone. Also, depending on your phones, RAM DVFS may not be supported. Please first check if your phone is available for RAM DVFS*

#### a. *Build*

```bash
cd scripts
sh build.sh
```

#### b. *Download Qwen1.5 0.5B*

Download the model from [here](https://huggingface.co/mllmTeam/qwen-1.5-0.5b-mllm/tree/main) and place the model file in the directory of `models`, or using the following instructions

```bash
mkdir ../models && cd ../models
# Download qwen-1.5-0.5b-q4_k.mllm
wget https://huggingface.co/mllmTeam/qwen-1.5-0.5b-mllm/resolve/main/qwen-1.5-0.5b-q4_k.mllm?download=true  -O qwen-1.5-0.5b-q4_k.mllm
```

#### c. *Run on Android phone*

<!-- ```bash
sh scripts-termux/run-setup.sh
sh scripts-termux/run.sh

# or -->
```bash
chmod +x scripts-termux/ignite-qwen.sh
su -c "taskset f0 scripts-termux/ignite-qwen.sh 10 10 6 6" # to control cpu core allocation
```


## ü™ê Customization

### Convert models

You can convert a pytorch/safetensor model to mllm model by yourself.

```bash
cd tools/convertor
pip install -r ./requirements.txt

# for one file pytorch model
python converter.py --input_model=model.pth --output_model=model.mllm --type=torch

# for multi-file pytorch model
python converter.py --input_model=pytorch_model.bin.index.json --output_model=model.mllm --type=torch

# for one file safetensor model
python converter.py --input_model=model.bin --output_model=model.mllm --type=safetensor

# for multi-file safetensor model
python converter.py --input_model=model.safetensors.index.json --output_model=model.mllm --type=safetensor
``` 

### Convert vocabulary

You can convert vocabulary to mllm vocabulary as followed.

```bash
cd tools/convertor
python vocab.py --input_file=tokenizer.json --output_file=vocab.mllm --type=Unigram
```

### Quantize models

You can quantize mllm model to int4 model by yourself.
mllm only support two quantize modes: Q4_0 and Q4_K.

```bash
cd bin
./quantize model.mllm model_q4_k.mllm Q4_K
```

## ‚ú® Acknowledgments

*IGNITE* project reuses the base kernels and implementation of [mllm](https://github.com/UbiquitousLearning/mllm).
Also, mllm reuses many low-level kernel implementation from [ggml](https://github.com/ggerganov/ggml) on ARM CPU.
It also utilizes [stb](https://github.com/nothings/stb) and [wenet](https://github.com/wenet-e2e/wenet) for
pre-processing images and audios.
mllm also has benefitted from following projects: [llama.cpp](https://github.com/ggerganov/llama.cpp)
and [MNN](https://github.com/alibaba/MNN).

## License

### Overall Project License

This project is licensed under the terms of the MIT License. Please see the [LICENSE](LICENSE) file in the root
directory for the full text of the MIT License.

### Apache 2.0 Licensed Components

Certain component([wenet](https://github.com/wenet-e2e/wenet)) of this project is licensed under the Apache License 2.0.
These component is clearly identified in their respective subdirectories along with a copy of the Apache License 2.0.
For the full text of the Apache License 2.0, please refer to the [LICENSE-APACHE](third_party/wenet_audio/LICENSE) file
located in the relevant subdirectories.